{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.077/IDS.147 Problem Set 5 <br>\n",
    "**Name:** Chun-Hei Lam. **ID:** 928931321 <br>\n",
    "**Declaration:** I pledge that the work submitted for this coursework is my own unassisted work unless stated otherwise. <br>\n",
    "**Acknowledgement to:** Harry Yu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.neighbors\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "misclass_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "def sign_loss(y_true, y_pred):\n",
    "    diff = 1-np.sum(np.abs(np.heaviside(y_true,0) - np.heaviside(y_pred,0)))/len(y_true)\n",
    "    return diff\n",
    "\n",
    "misclass_sign_scorer = make_scorer(sign_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTF 2.8 Handwriting Data\n",
    "Compare the classification performance of linear regression and k–nearest neighbor classification on the `zipcode` data. **In particular, consider only the $2$’s and $3$’s, and $k = 1, 3, 5, 7$ and $15$.** Show both the training and test error for each choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Solution: First preprocess the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"zip.train.gz\", delim_whitespace=True, header=None)\n",
    "train = train[(train[0] == 2) | (train[0] == 3)].sort_values(by=0)\n",
    "X_train = train.iloc[:, 1:]\n",
    "y_train = train[0].copy()\n",
    "y_train[y_train == 2] = -1\n",
    "y_train[y_train == 3] = 1\n",
    "\n",
    "test = pd.read_csv(\"zip.test.gz\", delim_whitespace=True, header=None)\n",
    "test = test[(test[0] == 2) | (test[0] == 3)].sort_values(by=0)\n",
    "X_test = test.iloc[:, 1:]\n",
    "y_test = test[0].copy()\n",
    "y_test[y_test == 2] = -1\n",
    "y_test[y_test == 3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Performance of linear classifier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score = 0.994240460763139, test score = 0.9587912087912088\n"
     ]
    }
   ],
   "source": [
    "linear_clf = sklearn.linear_model.RidgeClassifier(alpha=0).fit(X_train, y_train) #ridge with alpha = 0 is OLS\n",
    "train_score = linear_clf.score(X_train, y_train)\n",
    "test_score = linear_clf.score(X_test, y_test)\n",
    "print(f\"train score = {train_score}, test score = {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Performance of kNN classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, train score = 1.0, test score = 0.9752747252747253\n",
      "k = 3, train score = 0.9949604031677466, test score = 0.9697802197802198\n",
      "k = 5, train score = 0.994240460763139, test score = 0.9697802197802198\n",
      "k = 7, train score = 0.9935205183585313, test score = 0.967032967032967\n",
      "k = 15, train score = 0.9906407487401008, test score = 0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "for k in (1,3,5,7,15):\n",
    "    kNN_clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    train_score = kNN_clf.score(X_train, y_train)\n",
    "    test_score = kNN_clf.score(X_test, y_test)\n",
    "    print(f\"k = {k}, train score = {train_score}, test score = {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTF 7.5 Linear Smoother\n",
    "For a linear smoother $\\hat{\\vec{y}} = S\\vec{y}$, show that\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i, y_i) = \\text{tr}(S) \\sigma^2_\\epsilon\n",
    "\\end{equation}\n",
    "which justifies its use as the effective number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Solution: Just note that*\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i, y_i) = \\sum_{i=1}^N [\\text{Cov}(S\\vec{y}, \\vec{y})]_{ii} = \\sum_{i=1}^N [S\\text{Cov}(\\vec{y}, \\vec{y})]_{ii} = \\sum_{i=1}^N [S \\sigma^2_\\epsilon I]_{ii} = \\text{tr}(S) \\sigma^2_\\epsilon\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTF 3.17 Spam Data\n",
    "Estimated coefficients and test error results, for different subset and shrinkage methods applied to the `spam.txt` data. The blank entries correspond to variables omitted. **Best subset regression is NOT required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Solution: Preprocessing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam.txt\", delim_whitespace=True, header=None)\n",
    "X = df.iloc[:,:57].copy()\n",
    "# X = (X - X.mean())/X.std()\n",
    "y = df.iloc[:,57].copy()\n",
    "y[y==0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For each classifier return the coefficients and test errors for 10 folds CV.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score = 0.8500792228614543 +/- 0.11245239140272323\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "np.random.seed(seed)\n",
    "ols_clf = sklearn.linear_model.LinearRegression()\n",
    "ols_scores = cross_val_score(ols_clf, X, y, cv=10, scoring=misclass_sign_scorer)\n",
    "print(f\"test score = {ols_scores.mean()} +/- {ols_scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal alpha = 1.0, test score = 0.8826360464019617 +/- 0.01756715091770678\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "X = (X - X.mean())/X.std()\n",
    "np.random.seed(seed)\n",
    "ridge_clf = sklearn.linear_model.RidgeClassifierCV(alphas=np.logspace(-10,0,num=20))\n",
    "optim = ridge_clf.fit(X,y).alpha_\n",
    "\n",
    "ridge_clf_optim = sklearn.linear_model.RidgeClassifierCV(alphas=optim)\n",
    "ridge_scores = cross_val_score(ridge_clf_optim, X, y, cv=10, scoring=misclass_scorer)\n",
    "print(f\"optimal alpha = {optim}, test score = {ridge_scores.mean()} +/- {ridge_scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal alpha = 0.006579332246575682, test score = 0.8459520890314064 +/- 0.11838148218430437\n"
     ]
    }
   ],
   "source": [
    "# LASSO\n",
    "X = (X - X.mean())/X.std()\n",
    "np.random.seed(seed)\n",
    "lasso_clf = sklearn.linear_model.LassoCV(alphas=np.logspace(-4,-2,num=100), cv=10)\n",
    "optim = lasso_clf.fit(X,y).alpha_\n",
    "\n",
    "lasso_clf_optim = sklearn.linear_model.Lasso(alpha=optim)\n",
    "lasso_scores = cross_val_score(lasso_clf_optim, X, y, cv=10, scoring=misclass_sign_scorer)\n",
    "print(f\"optimal alpha = {optim}, test score = {lasso_scores.mean()} +/- {lasso_scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n_components = 2, test score = 0.8001028010940299 +/- 0.1589709259808129\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "X = (X - X.mean())/X.std()\n",
    "pcr_clf = GridSearchCV(PCA(), param_grid=dict(\n",
    "        n_components = [2,3,4,5,6,7,8,9],\n",
    "), cv=10).fit(X,y)\n",
    "optim = pcr_clf.best_params_[\"n_components\"]\n",
    "\n",
    "pcr_clf_optim =  make_pipeline(StandardScaler(), PCA(n_components=optim), sklearn.linear_model.LinearRegression())\n",
    "pcr_scores = cross_val_score(pcr_clf_optim, X, y, cv=10, scoring=misclass_sign_scorer)\n",
    "print(f\"optimal n_components = {optim}, test score = {pcr_scores.mean()} +/- {pcr_scores.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal n_components = 3, test score = -0.2756651931213928 +/- 0.8269955793641784\n"
     ]
    }
   ],
   "source": [
    "# PLS\n",
    "X = (X - X.mean())/X.std()\n",
    "PLS_clf = GridSearchCV(PLSRegression(), param_grid=dict(\n",
    "        n_components = [2,3,4,5,6,7,8,9],\n",
    "), cv=10).fit(X,y)\n",
    "optim = PLS_clf.best_params_[\"n_components\"]\n",
    "\n",
    "PLS_clf_optim = PLSRegression(n_components=optim)\n",
    "PLS_scores = cross_val_score(PLS_clf_optim, X, y, cv=10)\n",
    "print(f\"optimal n_components = {optim}, test score = {PLS_scores.mean()} +/- {PLS_scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTF 3.30 Elastic Net and Lasso\n",
    "Consider the elastic-net optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_\\beta \\left( \\|\\vec{y} - X\\vec{\\beta} \\|^2 + \\lambda \\left( \\alpha \\| \\beta \\|_2^2 + (1-\\alpha) \\|\\beta\\|_1 \\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Show how one can turn this into a lasso problem, using an augmented version of $X$ and $\\vec{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Solution: Let $\\tilde{X} = \\begin{pmatrix} X \\\\ \\sqrt{\\lambda \\alpha} I \\end{pmatrix}$ and $\\tilde{Y} = \\begin{pmatrix} \\vec{y} \\\\ \\vec{0} \\end{pmatrix}$. Then we have \n",
    "\\begin{equation}\n",
    "\\|\\tilde{y} - \\tilde{X}\\vec{\\beta} \\|^2_2 + \\lambda (1-\\alpha) \\|\\beta\\|_1 = \\| \\vec{y} - X\\vec{\\beta} \\|^2_2 + (\\sqrt{\\alpha \\lambda})^2 \\|\\vec{\\beta} \\|^2_2 + \\lambda (1-\\alpha) \\|\\beta\\|_1\n",
    "\\end{equation}*\n",
    "We have thus reduce an elastic net problem to a LASSO problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
